---
layout: paper
categories: papers
permalink: papers/auber
id: auber
title: "Auber: Automated BERT Regularization"
authors:
    - Hyun Dong Lee
    - Seongmin Lee
    - U Kang
equal-contribution:
    - Hyun Dong Lee
    - Seongmin Lee
venue: Public Library of Science
venue-shorthand: PLOS ONE
year: 2021
url: /papers/auber
pdf: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0253241
code: https://github.com/snudatalab/AUBER
type: journal
bibtex: |-

    @article{lee2021auber,
    title={Auber: automated bert regularization},
    author={Lee, Hyun Dong and Lee, Seongmin and Kang, U},
    journal={Plos one},
    volume={16},
    number={6},
    pages={e0253241},
    year={2021},
    publisher={Public Library of Science San Francisco, CA USA}
    }
---

How can we effectively regularize BERT? Although BERT proves its effectiveness in various NLP tasks, it often overfits when there are only a small number of training instances. A promising direction to regularize BERT is based on pruning its attention heads with a proxy score for head importance. However, these methods are usually suboptimal since they resort to arbitrarily determined numbers of attention heads to be pruned and do not directly aim for the performance enhancement. In order to overcome such a limitation, we propose AUBER, an automated BERT regularization method, that leverages reinforcement learning to automatically prune the proper attention heads from BERT. We also minimize the model complexity and the action search space by proposing a low-dimensional state representation and dually-greedy approach for training. Experimental results show that AUBER outperforms existing pruning methods by achieving up to 9.58% better performance. In addition, the ablation study demonstrates the effectiveness of design choices for AUBER.