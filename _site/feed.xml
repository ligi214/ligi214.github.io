<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-12-12T16:13:41-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Seongmin Lee</title><author><name>Seongmin Lee</name><email>seongmin@gatech.edu</email></author><entry><title type="html">High-Performance Transformers for Table Structure Recognition Need Early Convolutions</title><link href="http://localhost:4000/papers/tsr-convstem" rel="alternate" type="text/html" title="High-Performance Transformers for Table Structure Recognition Need Early Convolutions" /><published>2023-12-12T00:00:00-05:00</published><updated>2023-12-12T00:00:00-05:00</updated><id>http://localhost:4000/papers/tsr</id><content type="html" xml:base="http://localhost:4000/papers/tsr-convstem">&lt;p&gt;Table structure recognition (TSR) aims to convert tabular images into a machine- readable format, where a visual encoder extracts image features and a textual decoder generates table-representing tokens. Existing approaches use classic con- volutional neural network (CNN) backbones for the visual encoder and transform- ers for the textual decoder. However, this hybrid CNN-Transformer architecture introduces a complex visual encoder that accounts for nearly half of the total model parameters, markedly reduces both training and inference speed, and hinders the potential for self-supervised learning in TSR. In this work, we design a lightweight visual encoder for TSR without sacrificing expressive power. We discover that a convolutional stem can match classic CNN backbone performance, with a much simpler model. The convolutional stem strikes an optimal balance between two crucial factors for high-performance TSR: a higher receptive field (RF) ratio and a longer sequence length. This allows it to “see” an appropriate portion of the table and “store” the complex table structure within sufficient context length for the sub- sequent transformer. We conducted reproducible ablation studies and open-sourced our code at https://github.com/poloclub/tsr-convstem to enhance transparency, in- spire innovations, and facilitate fair comparisons in our domain as tables are a promising modality for representation learning.&lt;/p&gt;</content><author><name>ShengYun Peng</name></author><summary type="html">Table structure recognition (TSR) aims to convert tabular images into a machine- readable format, where a visual encoder extracts image features and a textual decoder generates table-representing tokens. Existing approaches use classic con- volutional neural network (CNN) backbones for the visual encoder and transform- ers for the textual decoder. However, this hybrid CNN-Transformer architecture introduces a complex visual encoder that accounts for nearly half of the total model parameters, markedly reduces both training and inference speed, and hinders the potential for self-supervised learning in TSR. In this work, we design a lightweight visual encoder for TSR without sacrificing expressive power. We discover that a convolutional stem can match classic CNN backbone performance, with a much simpler model. The convolutional stem strikes an optimal balance between two crucial factors for high-performance TSR: a higher receptive field (RF) ratio and a longer sequence length. This allows it to “see” an appropriate portion of the table and “store” the complex table structure within sufficient context length for the sub- sequent transformer. We conducted reproducible ablation studies and open-sourced our code at https://github.com/poloclub/tsr-convstem to enhance transparency, in- spire innovations, and facilitate fair comparisons in our domain as tables are a promising modality for representation learning.</summary></entry><entry><title type="html">Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion</title><link href="http://localhost:4000/papers/diffusion-explainer" rel="alternate" type="text/html" title="Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion" /><published>2023-10-02T00:00:00-04:00</published><updated>2023-10-02T00:00:00-04:00</updated><id>http://localhost:4000/papers/diffusionexplainer</id><content type="html" xml:base="http://localhost:4000/papers/diffusion-explainer">&lt;p&gt;Diffusion-based generative models’ impressive ability to create convincing images has captured global attention. However, their complex internal structures and operations often make them difficult for non-experts to understand. We present Diffusion Explainer, the first interactive visualization tool that explains how Stable Diffusion transforms text prompts into images. Diffusion Explainer tightly integrates a visual overview of Stable Diffusion’s complex components with detailed explanations of their underlying operations, enabling users to fluidly transition between multiple levels of abstraction through animations and interactive elements. By comparing the evolutions of image representations guided by two related text prompts over refinement timesteps, users can discover the impact of prompts on image generation. Diffusion Explainer runs locally in users’ web browsers without the need for installation or specialized hardware, broadening the public’s education access to modern AI techniques. Our open-sourced tool is available at: https://poloclub.github.io/diffusion-explainer/. A video demo is available at https://youtu.be/Zg4gxdIWDds.&lt;/p&gt;</content><author><name>Seongmin Lee</name></author><summary type="html">Diffusion-based generative models’ impressive ability to create convincing images has captured global attention. However, their complex internal structures and operations often make them difficult for non-experts to understand. We present Diffusion Explainer, the first interactive visualization tool that explains how Stable Diffusion transforms text prompts into images. Diffusion Explainer tightly integrates a visual overview of Stable Diffusion’s complex components with detailed explanations of their underlying operations, enabling users to fluidly transition between multiple levels of abstraction through animations and interactive elements. By comparing the evolutions of image representations guided by two related text prompts over refinement timesteps, users can discover the impact of prompts on image generation. Diffusion Explainer runs locally in users’ web browsers without the need for installation or specialized hardware, broadening the public’s education access to modern AI techniques. Our open-sourced tool is available at: https://poloclub.github.io/diffusion-explainer/. A video demo is available at https://youtu.be/Zg4gxdIWDds.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/featured/23-diffusion-explainer-short.gif" /><media:content medium="image" url="http://localhost:4000/images/featured/23-diffusion-explainer-short.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">VisGrader: Automatic Grading of D3 Visualizations</title><link href="http://localhost:4000/papers/visgrader" rel="alternate" type="text/html" title="VisGrader: Automatic Grading of D3 Visualizations" /><published>2023-10-01T00:00:00-04:00</published><updated>2023-10-01T00:00:00-04:00</updated><id>http://localhost:4000/papers/visgrader</id><content type="html" xml:base="http://localhost:4000/papers/visgrader">&lt;p&gt;Manually grading D3 data visualizations is a challenging endeavor, and is especially difficult for large classes with hundreds of students. Grading an interactive visualization requires a combination of interactive, quantitative, and qualitative evaluation that are conventionally done manually and are difficult to scale up as the visualization complexity, data size, and number of students increase. We present VisGrader, a first-of-its kind automatic grading method for D3 visualizations that scalably and precisely evaluates the data bindings, visual encodings, interactions, and design specifications used in a visualization. Our method enhances students’ learning experience, enabling them to submit their code frequently and receive rapid feedback to better inform iteration and improvement to their code and visualization design. We have successfully deployed our method and auto-graded D3 submissions from more than 4000 students in a visualization course at Georgia Tech, and received positive feedback for expanding its adoption.&lt;/p&gt;</content><author><name>Matthew Hull</name></author><summary type="html">Manually grading D3 data visualizations is a challenging endeavor, and is especially difficult for large classes with hundreds of students. Grading an interactive visualization requires a combination of interactive, quantitative, and qualitative evaluation that are conventionally done manually and are difficult to scale up as the visualization complexity, data size, and number of students increase. We present VisGrader, a first-of-its kind automatic grading method for D3 visualizations that scalably and precisely evaluates the data bindings, visual encodings, interactions, and design specifications used in a visualization. Our method enhances students’ learning experience, enabling them to submit their code frequently and receive rapid feedback to better inform iteration and improvement to their code and visualization design. We have successfully deployed our method and auto-graded D3 submissions from more than 4000 students in a visualization course at Georgia Tech, and received positive feedback for expanding its adoption.</summary></entry><entry><title type="html">Concept Evolution in Deep Learning Training: A Unified Interpretation Framework and Discoveries</title><link href="http://localhost:4000/papers/conceptevo" rel="alternate" type="text/html" title="Concept Evolution in Deep Learning Training: A Unified Interpretation Framework and Discoveries" /><published>2023-10-01T00:00:00-04:00</published><updated>2023-10-01T00:00:00-04:00</updated><id>http://localhost:4000/papers/conceptevo</id><content type="html" xml:base="http://localhost:4000/papers/conceptevo">&lt;p&gt;We present ConceptEvo, a unified interpretation framework for deep neural networks (DNNs) that reveals the inception and evolution of learned concepts during training. Our work addresses a critical gap in DNN interpretation research, as existing methods primarily focus on post-training interpretation. ConceptEvo introduces two novel technical contributions: (1) an algorithm that generates a unified semantic space, enabling side-by-side comparison of different models during training, and (2) an algorithm that discovers and quantifies important concept evolutions for class predictions. Through a large-scale human evaluation and quantitative experiments, we demonstrate that ConceptEvo successfully identifies concept evolutions across different models, which are not only comprehensible to humans but also crucial for class predictions. ConceptEvo is applicable to both modern DNN architectures, such as ConvNeXt, and classic DNNs, such as VGGs and InceptionV3.&lt;/p&gt;</content><author><name>Haekyu Park</name></author><summary type="html">We present ConceptEvo, a unified interpretation framework for deep neural networks (DNNs) that reveals the inception and evolution of learned concepts during training. Our work addresses a critical gap in DNN interpretation research, as existing methods primarily focus on post-training interpretation. ConceptEvo introduces two novel technical contributions: (1) an algorithm that generates a unified semantic space, enabling side-by-side comparison of different models during training, and (2) an algorithm that discovers and quantifies important concept evolutions for class predictions. Through a large-scale human evaluation and quantitative experiments, we demonstrate that ConceptEvo successfully identifies concept evolutions across different models, which are not only comprehensible to humans but also crucial for class predictions. ConceptEvo is applicable to both modern DNN architectures, such as ConvNeXt, and classic DNNs, such as VGGs and InceptionV3.</summary></entry><entry><title type="html">Towards Mitigating Spurious Correlations in Image Classifiers with Simple Yes-no Feedback</title><link href="http://localhost:4000/papers/crayon" rel="alternate" type="text/html" title="Towards Mitigating Spurious Correlations in Image Classifiers with Simple Yes-no Feedback" /><published>2023-07-30T00:00:00-04:00</published><updated>2023-07-30T00:00:00-04:00</updated><id>http://localhost:4000/papers/crayon</id><content type="html" xml:base="http://localhost:4000/papers/crayon">&lt;p&gt;Modern deep learning models have achieved remarkable performance. However, they often rely on spurious correlations between data and labels that exist only in the training data, resulting in poor generalization performance. We present CRAYON (Correlation Rectification Algorithms by Yes Or No), effective, scalable, and practical solutions to refine models with spurious correlations using simple yes-no feedback on model interpretations. CRAYON addresses key limitations of existing approaches that heavily rely on costly human intervention and empowers popular model interpretation techniques to mitigate spurious correlations in two distinct ways: CRAYON-ATTENTION guides saliency maps to focus on relevant image regions, and CRAYON-PRUNING prunes irrelevant neurons to remove their influence. Extensive evaluation on three benchmark image datasets and three state-of-the-art methods demonstrates that our methods effectively mitigate spurious correlations, achieving comparable or even better performance than existing approaches that require more complex feedback.&lt;/p&gt;</content><author><name>Seongmin Lee</name></author><summary type="html">Modern deep learning models have achieved remarkable performance. However, they often rely on spurious correlations between data and labels that exist only in the training data, resulting in poor generalization performance. We present CRAYON (Correlation Rectification Algorithms by Yes Or No), effective, scalable, and practical solutions to refine models with spurious correlations using simple yes-no feedback on model interpretations. CRAYON addresses key limitations of existing approaches that heavily rely on costly human intervention and empowers popular model interpretation techniques to mitigate spurious correlations in two distinct ways: CRAYON-ATTENTION guides saliency maps to focus on relevant image regions, and CRAYON-PRUNING prunes irrelevant neurons to remove their influence. Extensive evaluation on three benchmark image datasets and three state-of-the-art methods demonstrates that our methods effectively mitigate spurious correlations, achieving comparable or even better performance than existing approaches that require more complex feedback.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/featured/23-crayon.png" /><media:content medium="image" url="http://localhost:4000/images/featured/23-crayon.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">SuperNOVA: Design Strategies and Opportunities for Interactive Visualization in Computational Notebooks</title><link href="http://localhost:4000/papers/supernova" rel="alternate" type="text/html" title="SuperNOVA: Design Strategies and Opportunities for Interactive Visualization in Computational Notebooks" /><published>2023-05-04T00:00:00-04:00</published><updated>2023-05-04T00:00:00-04:00</updated><id>http://localhost:4000/papers/supernova</id><content type="html" xml:base="http://localhost:4000/papers/supernova">&lt;p&gt;Computational notebooks such as Jupyter Notebook have become data scientists’ de facto programming environments. Many visualization researchers and practitioners have developed interactive visualization tools that support notebooks. However, little is known about the appropriate design of visual analytics (VA) tools in notebooks. To bridge this critical research gap, we investigate the design strategies in this space by analyzing 161 notebook VA tools and their users’ feedback. Our analysis encompasses 64 systems from academic papers and 103 systems sourced from a pool of 55k notebooks containing interactive visualizations that we obtain via scraping 8.6 million notebooks on GitHub. We also examine findings from 15 user studies and user feedback in 379 GitHub issues. Through this work, we identify unique design opportunities and considerations for future notebook VA tools, such as using and manipulating multimodal data in notebooks as well as balancing the degree of visualization-notebook integration. Finally, we develop SUPERNOVA, an open-source interactive tool to help researchers explore existing notebook VA tools and search for related work.&lt;/p&gt;</content><author><name>Zijie J. Wang</name></author><summary type="html">Computational notebooks such as Jupyter Notebook have become data scientists’ de facto programming environments. Many visualization researchers and practitioners have developed interactive visualization tools that support notebooks. However, little is known about the appropriate design of visual analytics (VA) tools in notebooks. To bridge this critical research gap, we investigate the design strategies in this space by analyzing 161 notebook VA tools and their users’ feedback. Our analysis encompasses 64 systems from academic papers and 103 systems sourced from a pool of 55k notebooks containing interactive visualizations that we obtain via scraping 8.6 million notebooks on GitHub. We also examine findings from 15 user studies and user feedback in 379 GitHub issues. Through this work, we identify unique design opportunities and considerations for future notebook VA tools, such as using and manipulating multimodal data in notebooks as well as balancing the degree of visualization-notebook integration. Finally, we develop SUPERNOVA, an open-source interactive tool to help researchers explore existing notebook VA tools and search for related work.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/papers/23-supernova.png" /><media:content medium="image" url="http://localhost:4000/images/papers/23-supernova.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Explaining Website Reliability by Visualizing Hyperlink Connectivity</title><link href="http://localhost:4000/papers/misvis" rel="alternate" type="text/html" title="Explaining Website Reliability by Visualizing Hyperlink Connectivity" /><published>2022-10-16T00:00:00-04:00</published><updated>2022-10-16T00:00:00-04:00</updated><id>http://localhost:4000/papers/misvis</id><content type="html" xml:base="http://localhost:4000/papers/misvis">&lt;p&gt;As the information on the Internet continues growing exponentially, understanding and assessing the reliability of a website is becoming increasingly important. Misinformation has far-ranging repercussions, from sowing mistrust in media to undermining democratic elections. While some research investigates how to alert people to misinformation on the web, much less research has been conducted on explaining how websites engage in spreading false information. To fill the research gap, we present MISVIS, a web-based interactive visualization tool that helps users assess a website’s reliability by understanding how it engages in spreading false information on the World Wide Web. MISVIS visualizes the hyperlink connectivity of the website and summarizes key characteristics of the Twitter accounts that mention the site. A large-scale user study with 139 participants demonstrates that MISVIS facilitates users to assess and understand false information on the web and node-link diagrams can be used to communicate with non-experts. MISVIS is available at the public demo link: https://poloclub.github.io/MisVis.&lt;/p&gt;</content><author><name>Seongmin Lee</name></author><summary type="html">As the information on the Internet continues growing exponentially, understanding and assessing the reliability of a website is becoming increasingly important. Misinformation has far-ranging repercussions, from sowing mistrust in media to undermining democratic elections. While some research investigates how to alert people to misinformation on the web, much less research has been conducted on explaining how websites engage in spreading false information. To fill the research gap, we present MISVIS, a web-based interactive visualization tool that helps users assess a website’s reliability by understanding how it engages in spreading false information on the World Wide Web. MISVIS visualizes the hyperlink connectivity of the website and summarizes key characteristics of the Twitter accounts that mention the site. A large-scale user study with 139 participants demonstrates that MISVIS facilitates users to assess and understand false information on the web and node-link diagrams can be used to communicate with non-experts. MISVIS is available at the public demo link: https://poloclub.github.io/MisVis.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/featured/22-misvis.gif" /><media:content medium="image" url="http://localhost:4000/images/featured/22-misvis.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">NOVA: A Practical Method for Creating Notebook-Ready Visual Analytics</title><link href="http://localhost:4000/papers/nova" rel="alternate" type="text/html" title="NOVA: A Practical Method for Creating Notebook-Ready Visual Analytics" /><published>2022-10-15T00:00:00-04:00</published><updated>2022-10-15T00:00:00-04:00</updated><id>http://localhost:4000/papers/nova</id><content type="html" xml:base="http://localhost:4000/papers/nova">&lt;p&gt;How can we develop visual analytics (VA) tools that can be easily adopted? Visualization researchers have developed a large number of web-based VA tools to help data scientists in a wide range of tasks. However, adopting these standalone systems can be challenging, as they require data scientists to create new workflows to streamline the VA processes. Recent surveys suggest computational notebooks have been dominating data scientists’ analytical workflows, as these notebooks seamlessly combine text, code, and visualization, allowing users to rapidly iterate code experiments. To help visualization researchers develop VA tools that can be easily integrated into existing data science workflows, we present NOVA, a simple and flexible method to adapt web-based VA systems for notebooks. We provide detailed examples of using this method with diverse web development technologies and different types of computational notebooks. Deployed application examples highlight that NOVA is easy to adopt, and data scientists appreciate in-notebook VA. NOVA is available at https://github.com/poloclub/nova.&lt;/p&gt;</content><author><name>Zijie J. Wang</name></author><summary type="html">How can we develop visual analytics (VA) tools that can be easily adopted? Visualization researchers have developed a large number of web-based VA tools to help data scientists in a wide range of tasks. However, adopting these standalone systems can be challenging, as they require data scientists to create new workflows to streamline the VA processes. Recent surveys suggest computational notebooks have been dominating data scientists’ analytical workflows, as these notebooks seamlessly combine text, code, and visualization, allowing users to rapidly iterate code experiments. To help visualization researchers develop VA tools that can be easily integrated into existing data science workflows, we present NOVA, a simple and flexible method to adapt web-based VA systems for notebooks. We provide detailed examples of using this method with diverse web development technologies and different types of computational notebooks. Deployed application examples highlight that NOVA is easy to adopt, and data scientists appreciate in-notebook VA. NOVA is available at https://github.com/poloclub/nova.</summary></entry><entry><title type="html">VisCUIT: Visual Auditor for Bias in CNN Image Classifier</title><link href="http://localhost:4000/papers/viscuit" rel="alternate" type="text/html" title="VisCUIT: Visual Auditor for Bias in CNN Image Classifier" /><published>2022-06-18T00:00:00-04:00</published><updated>2022-06-18T00:00:00-04:00</updated><id>http://localhost:4000/papers/viscuit</id><content type="html" xml:base="http://localhost:4000/papers/viscuit">&lt;p&gt;CNN image classifiers are widely used, thanks to their efficiency and accuracy. However, they can suffer from biases that impede their practical applications. Most existing bias investigation techniques are either inapplicable to general image classification tasks or require significant user efforts in perusing all data subgroups to manually specify which data attributes to inspect. We present VISCUIT, an interactive visualization system that reveals how and why a CNN classifier is biased. VISCUIT visually summarizes the subgroups on which the classifier underperforms and helps users discover and characterize the cause of the underperformances by revealing image concepts responsible for activating neurons that contribute to misclassifications. VISCUIT runs in modern browsers and is open-source, allowing people to easily access and extend the tool to other model architectures and datasets. VISCUIT is available at the following public demo link: https: //poloclub.github.io/VisCUIT . A video demo is available at https://youtu.be/eNDbSyM4R_4.&lt;/p&gt;</content><author><name>Seongmin Lee</name></author><summary type="html">CNN image classifiers are widely used, thanks to their efficiency and accuracy. However, they can suffer from biases that impede their practical applications. Most existing bias investigation techniques are either inapplicable to general image classification tasks or require significant user efforts in perusing all data subgroups to manually specify which data attributes to inspect. We present VISCUIT, an interactive visualization system that reveals how and why a CNN classifier is biased. VISCUIT visually summarizes the subgroups on which the classifier underperforms and helps users discover and characterize the cause of the underperformances by revealing image concepts responsible for activating neurons that contribute to misclassifications. VISCUIT runs in modern browsers and is open-source, allowing people to easily access and extend the tool to other model architectures and datasets. VISCUIT is available at the following public demo link: https: //poloclub.github.io/VisCUIT . A video demo is available at https://youtu.be/eNDbSyM4R_4.</summary></entry><entry><title type="html">MisVis: Explaining Web Misinformation Connections via Visual Summary</title><link href="http://localhost:4000/papers/misvis_chi" rel="alternate" type="text/html" title="MisVis: Explaining Web Misinformation Connections via Visual Summary" /><published>2022-04-27T00:00:00-04:00</published><updated>2022-04-27T00:00:00-04:00</updated><id>http://localhost:4000/papers/misvis_chi</id><content type="html" xml:base="http://localhost:4000/papers/misvis_chi">&lt;p&gt;Identifying and raising awareness about web misinformation is crucial as the Internet has become a major source of information for many people. We introduce MisVis, a web-based interactive tool that helps users better assess misinformation websites and understand their connections with other misinformation sites through visual explanations. Different from the existing techniques that primarily only focus on alerting users of misinformation, MisVis provides new ways to visualize how the site is involved in spreading information on the web and social media. Through MisVis, we contribute novel interactive visual design: Summary View helps users understand a site’s overall reliability by showing the distributions of its linked websites; Graph View presents users with the connection details of how a site is linked to other misinformation websites. In collaboration with researchers at a large security company, we are working to deploy MisVis as a web browser extension for broader impact.&lt;/p&gt;</content><author><name>Seongmin Lee</name></author><summary type="html">Identifying and raising awareness about web misinformation is crucial as the Internet has become a major source of information for many people. We introduce MisVis, a web-based interactive tool that helps users better assess misinformation websites and understand their connections with other misinformation sites through visual explanations. Different from the existing techniques that primarily only focus on alerting users of misinformation, MisVis provides new ways to visualize how the site is involved in spreading information on the web and social media. Through MisVis, we contribute novel interactive visual design: Summary View helps users understand a site’s overall reliability by showing the distributions of its linked websites; Graph View presents users with the connection details of how a site is linked to other misinformation websites. In collaboration with researchers at a large security company, we are working to deploy MisVis as a web browser extension for broader impact.</summary></entry></feed>