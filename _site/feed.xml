<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-10-07T21:36:06-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Seongmin Lee</title><author><name>Seongmin Lee</name><email>seongmin@gatech.edu</email></author><entry><title type="html">Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion</title><link href="http://localhost:4000/papers/diffusion-explainer" rel="alternate" type="text/html" title="Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion" /><published>2023-10-02T00:00:00-04:00</published><updated>2023-10-02T00:00:00-04:00</updated><id>http://localhost:4000/papers/diffusionexplainer</id><content type="html" xml:base="http://localhost:4000/papers/diffusion-explainer">&lt;p&gt;Diffusion-based generative models’ impressive ability to create convincing images has captured global attention. However, their complex internal structures and operations often make them difficult for non-experts to understand. We present Diffusion Explainer, the first interactive visualization tool that explains how Stable Diffusion transforms text prompts into images. Diffusion Explainer tightly integrates a visual overview of Stable Diffusion’s complex components with detailed explanations of their underlying operations, enabling users to fluidly transition between multiple levels of abstraction through animations and interactive elements. By comparing the evolutions of image representations guided by two related text prompts over refinement timesteps, users can discover the impact of prompts on image generation. Diffusion Explainer runs locally in users’ web browsers without the need for installation or specialized hardware, broadening the public’s education access to modern AI techniques. Our open-sourced tool is available at: https://poloclub.github.io/diffusion-explainer/. A video demo is available at https://youtu.be/Zg4gxdIWDds.&lt;/p&gt;</content><author><name>Seongmin Lee</name></author><summary type="html">Diffusion-based generative models’ impressive ability to create convincing images has captured global attention. However, their complex internal structures and operations often make them difficult for non-experts to understand. We present Diffusion Explainer, the first interactive visualization tool that explains how Stable Diffusion transforms text prompts into images. Diffusion Explainer tightly integrates a visual overview of Stable Diffusion’s complex components with detailed explanations of their underlying operations, enabling users to fluidly transition between multiple levels of abstraction through animations and interactive elements. By comparing the evolutions of image representations guided by two related text prompts over refinement timesteps, users can discover the impact of prompts on image generation. Diffusion Explainer runs locally in users’ web browsers without the need for installation or specialized hardware, broadening the public’s education access to modern AI techniques. Our open-sourced tool is available at: https://poloclub.github.io/diffusion-explainer/. A video demo is available at https://youtu.be/Zg4gxdIWDds.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/featured/23-diffusion-explainer-short.gif" /><media:content medium="image" url="http://localhost:4000/images/featured/23-diffusion-explainer-short.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">VisGrader: Automatic Grading of D3 Visualizations</title><link href="http://localhost:4000/papers/visgrader" rel="alternate" type="text/html" title="VisGrader: Automatic Grading of D3 Visualizations" /><published>2023-10-01T00:00:00-04:00</published><updated>2023-10-01T00:00:00-04:00</updated><id>http://localhost:4000/papers/visgrader</id><content type="html" xml:base="http://localhost:4000/papers/visgrader">&lt;p&gt;Manually grading D3 data visualizations is a challenging endeavor, and is especially difficult for large classes with hundreds of students. Grading an interactive visualization requires a combination of interactive, quantitative, and qualitative evaluation that are conventionally done manually and are difficult to scale up as the visualization complexity, data size, and number of students increase. We present VisGrader, a first-of-its kind automatic grading method for D3 visualizations that scalably and precisely evaluates the data bindings, visual encodings, interactions, and design specifications used in a visualization. Our method enhances students’ learning experience, enabling them to submit their code frequently and receive rapid feedback to better inform iteration and improvement to their code and visualization design. We have successfully deployed our method and auto-graded D3 submissions from more than 4000 students in a visualization course at Georgia Tech, and received positive feedback for expanding its adoption.&lt;/p&gt;</content><author><name>Matthew Hull</name></author><summary type="html">Manually grading D3 data visualizations is a challenging endeavor, and is especially difficult for large classes with hundreds of students. Grading an interactive visualization requires a combination of interactive, quantitative, and qualitative evaluation that are conventionally done manually and are difficult to scale up as the visualization complexity, data size, and number of students increase. We present VisGrader, a first-of-its kind automatic grading method for D3 visualizations that scalably and precisely evaluates the data bindings, visual encodings, interactions, and design specifications used in a visualization. Our method enhances students’ learning experience, enabling them to submit their code frequently and receive rapid feedback to better inform iteration and improvement to their code and visualization design. We have successfully deployed our method and auto-graded D3 submissions from more than 4000 students in a visualization course at Georgia Tech, and received positive feedback for expanding its adoption.</summary></entry><entry><title type="html">Concept Evolution in Deep Learning Training: A Unified Interpretation Framework and Discoveries</title><link href="http://localhost:4000/papers/conceptevo" rel="alternate" type="text/html" title="Concept Evolution in Deep Learning Training: A Unified Interpretation Framework and Discoveries" /><published>2023-10-01T00:00:00-04:00</published><updated>2023-10-01T00:00:00-04:00</updated><id>http://localhost:4000/papers/conceptevo</id><content type="html" xml:base="http://localhost:4000/papers/conceptevo">&lt;p&gt;We present ConceptEvo, a unified interpretation framework for deep neural networks (DNNs) that reveals the inception and evolution of learned concepts during training. Our work addresses a critical gap in DNN interpretation research, as existing methods primarily focus on post-training interpretation. ConceptEvo introduces two novel technical contributions: (1) an algorithm that generates a unified semantic space, enabling side-by-side comparison of different models during training, and (2) an algorithm that discovers and quantifies important concept evolutions for class predictions. Through a large-scale human evaluation and quantitative experiments, we demonstrate that ConceptEvo successfully identifies concept evolutions across different models, which are not only comprehensible to humans but also crucial for class predictions. ConceptEvo is applicable to both modern DNN architectures, such as ConvNeXt, and classic DNNs, such as VGGs and InceptionV3.&lt;/p&gt;</content><author><name>Haekyu Park</name></author><summary type="html">We present ConceptEvo, a unified interpretation framework for deep neural networks (DNNs) that reveals the inception and evolution of learned concepts during training. Our work addresses a critical gap in DNN interpretation research, as existing methods primarily focus on post-training interpretation. ConceptEvo introduces two novel technical contributions: (1) an algorithm that generates a unified semantic space, enabling side-by-side comparison of different models during training, and (2) an algorithm that discovers and quantifies important concept evolutions for class predictions. Through a large-scale human evaluation and quantitative experiments, we demonstrate that ConceptEvo successfully identifies concept evolutions across different models, which are not only comprehensible to humans but also crucial for class predictions. ConceptEvo is applicable to both modern DNN architectures, such as ConvNeXt, and classic DNNs, such as VGGs and InceptionV3.</summary></entry><entry><title type="html">Towards Mitigating Spurious Correlations in Image Classifiers with Simple Yes-no Feedback</title><link href="http://localhost:4000/papers/crayon" rel="alternate" type="text/html" title="Towards Mitigating Spurious Correlations in Image Classifiers with Simple Yes-no Feedback" /><published>2023-07-30T00:00:00-04:00</published><updated>2023-07-30T00:00:00-04:00</updated><id>http://localhost:4000/papers/crayon</id><content type="html" xml:base="http://localhost:4000/papers/crayon">&lt;p&gt;Modern deep learning models have achieved remarkable performance. However, they often rely on spurious correlations between data and labels that exist only in the training data, resulting in poor generalization performance. We present CRAYON (Correlation Rectification Algorithms by Yes Or No), effective, scalable, and practical solutions to refine models with spurious correlations using simple yes-no feedback on model interpretations. CRAYON addresses key limitations of existing approaches that heavily rely on costly human intervention and empowers popular model interpretation techniques to mitigate spurious correlations in two distinct ways: CRAYON-ATTENTION guides saliency maps to focus on relevant image regions, and CRAYON-PRUNING prunes irrelevant neurons to remove their influence. Extensive evaluation on three benchmark image datasets and three state-of-the-art methods demonstrates that our methods effectively mitigate spurious correlations, achieving comparable or even better performance than existing approaches that require more complex feedback.&lt;/p&gt;</content><author><name>Seongmin Lee</name></author><summary type="html">Modern deep learning models have achieved remarkable performance. However, they often rely on spurious correlations between data and labels that exist only in the training data, resulting in poor generalization performance. We present CRAYON (Correlation Rectification Algorithms by Yes Or No), effective, scalable, and practical solutions to refine models with spurious correlations using simple yes-no feedback on model interpretations. CRAYON addresses key limitations of existing approaches that heavily rely on costly human intervention and empowers popular model interpretation techniques to mitigate spurious correlations in two distinct ways: CRAYON-ATTENTION guides saliency maps to focus on relevant image regions, and CRAYON-PRUNING prunes irrelevant neurons to remove their influence. Extensive evaluation on three benchmark image datasets and three state-of-the-art methods demonstrates that our methods effectively mitigate spurious correlations, achieving comparable or even better performance than existing approaches that require more complex feedback.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/featured/23-crayon.png" /><media:content medium="image" url="http://localhost:4000/images/featured/23-crayon.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">SuperNOVA: Design Strategies and Opportunities for Interactive Visualization in Computational Notebooks</title><link href="http://localhost:4000/papers/supernova" rel="alternate" type="text/html" title="SuperNOVA: Design Strategies and Opportunities for Interactive Visualization in Computational Notebooks" /><published>2023-05-04T00:00:00-04:00</published><updated>2023-05-04T00:00:00-04:00</updated><id>http://localhost:4000/papers/supernova</id><content type="html" xml:base="http://localhost:4000/papers/supernova">&lt;p&gt;Computational notebooks such as Jupyter Notebook have become data scientists’ de facto programming environments. Many visualization researchers and practitioners have developed interactive visualization tools that support notebooks. However, little is known about the appropriate design of visual analytics (VA) tools in notebooks. To bridge this critical research gap, we investigate the design strategies in this space by analyzing 161 notebook VA tools and their users’ feedback. Our analysis encompasses 64 systems from academic papers and 103 systems sourced from a pool of 55k notebooks containing interactive visualizations that we obtain via scraping 8.6 million notebooks on GitHub. We also examine findings from 15 user studies and user feedback in 379 GitHub issues. Through this work, we identify unique design opportunities and considerations for future notebook VA tools, such as using and manipulating multimodal data in notebooks as well as balancing the degree of visualization-notebook integration. Finally, we develop SUPERNOVA, an open-source interactive tool to help researchers explore existing notebook VA tools and search for related work.&lt;/p&gt;</content><author><name>Zijie J. Wang</name></author><summary type="html">Computational notebooks such as Jupyter Notebook have become data scientists’ de facto programming environments. Many visualization researchers and practitioners have developed interactive visualization tools that support notebooks. However, little is known about the appropriate design of visual analytics (VA) tools in notebooks. To bridge this critical research gap, we investigate the design strategies in this space by analyzing 161 notebook VA tools and their users’ feedback. Our analysis encompasses 64 systems from academic papers and 103 systems sourced from a pool of 55k notebooks containing interactive visualizations that we obtain via scraping 8.6 million notebooks on GitHub. We also examine findings from 15 user studies and user feedback in 379 GitHub issues. Through this work, we identify unique design opportunities and considerations for future notebook VA tools, such as using and manipulating multimodal data in notebooks as well as balancing the degree of visualization-notebook integration. Finally, we develop SUPERNOVA, an open-source interactive tool to help researchers explore existing notebook VA tools and search for related work.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/papers/23-supernova.png" /><media:content medium="image" url="http://localhost:4000/images/papers/23-supernova.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Explaining Website Reliability by Visualizing Hyperlink Connectivity</title><link href="http://localhost:4000/papers/misvis" rel="alternate" type="text/html" title="Explaining Website Reliability by Visualizing Hyperlink Connectivity" /><published>2022-10-16T00:00:00-04:00</published><updated>2022-10-16T00:00:00-04:00</updated><id>http://localhost:4000/papers/misvis</id><content type="html" xml:base="http://localhost:4000/papers/misvis">&lt;p&gt;As the information on the Internet continues growing exponentially, understanding and assessing the reliability of a website is becoming increasingly important. Misinformation has far-ranging repercussions, from sowing mistrust in media to undermining democratic elections. While some research investigates how to alert people to misinformation on the web, much less research has been conducted on explaining how websites engage in spreading false information. To fill the research gap, we present MISVIS, a web-based interactive visualization tool that helps users assess a website’s reliability by understanding how it engages in spreading false information on the World Wide Web. MISVIS visualizes the hyperlink connectivity of the website and summarizes key characteristics of the Twitter accounts that mention the site. A large-scale user study with 139 participants demonstrates that MISVIS facilitates users to assess and understand false information on the web and node-link diagrams can be used to communicate with non-experts. MISVIS is available at the public demo link: https://poloclub.github.io/MisVis.&lt;/p&gt;</content><author><name>Seongmin Lee</name></author><summary type="html">As the information on the Internet continues growing exponentially, understanding and assessing the reliability of a website is becoming increasingly important. Misinformation has far-ranging repercussions, from sowing mistrust in media to undermining democratic elections. While some research investigates how to alert people to misinformation on the web, much less research has been conducted on explaining how websites engage in spreading false information. To fill the research gap, we present MISVIS, a web-based interactive visualization tool that helps users assess a website’s reliability by understanding how it engages in spreading false information on the World Wide Web. MISVIS visualizes the hyperlink connectivity of the website and summarizes key characteristics of the Twitter accounts that mention the site. A large-scale user study with 139 participants demonstrates that MISVIS facilitates users to assess and understand false information on the web and node-link diagrams can be used to communicate with non-experts. MISVIS is available at the public demo link: https://poloclub.github.io/MisVis.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/featured/22-misvis.gif" /><media:content medium="image" url="http://localhost:4000/images/featured/22-misvis.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">NOVA: A Practical Method for Creating Notebook-Ready Visual Analytics</title><link href="http://localhost:4000/papers/nova" rel="alternate" type="text/html" title="NOVA: A Practical Method for Creating Notebook-Ready Visual Analytics" /><published>2022-10-15T00:00:00-04:00</published><updated>2022-10-15T00:00:00-04:00</updated><id>http://localhost:4000/papers/nova</id><content type="html" xml:base="http://localhost:4000/papers/nova">&lt;p&gt;How can we develop visual analytics (VA) tools that can be easily adopted? Visualization researchers have developed a large number of web-based VA tools to help data scientists in a wide range of tasks. However, adopting these standalone systems can be challenging, as they require data scientists to create new workflows to streamline the VA processes. Recent surveys suggest computational notebooks have been dominating data scientists’ analytical workflows, as these notebooks seamlessly combine text, code, and visualization, allowing users to rapidly iterate code experiments. To help visualization researchers develop VA tools that can be easily integrated into existing data science workflows, we present NOVA, a simple and flexible method to adapt web-based VA systems for notebooks. We provide detailed examples of using this method with diverse web development technologies and different types of computational notebooks. Deployed application examples highlight that NOVA is easy to adopt, and data scientists appreciate in-notebook VA. NOVA is available at https://github.com/poloclub/nova.&lt;/p&gt;</content><author><name>Zijie J. Wang</name></author><summary type="html">How can we develop visual analytics (VA) tools that can be easily adopted? Visualization researchers have developed a large number of web-based VA tools to help data scientists in a wide range of tasks. However, adopting these standalone systems can be challenging, as they require data scientists to create new workflows to streamline the VA processes. Recent surveys suggest computational notebooks have been dominating data scientists’ analytical workflows, as these notebooks seamlessly combine text, code, and visualization, allowing users to rapidly iterate code experiments. To help visualization researchers develop VA tools that can be easily integrated into existing data science workflows, we present NOVA, a simple and flexible method to adapt web-based VA systems for notebooks. We provide detailed examples of using this method with diverse web development technologies and different types of computational notebooks. Deployed application examples highlight that NOVA is easy to adopt, and data scientists appreciate in-notebook VA. NOVA is available at https://github.com/poloclub/nova.</summary></entry><entry><title type="html">VisCUIT: Visual Auditor for Bias in CNN Image Classifier</title><link href="http://localhost:4000/papers/viscuit" rel="alternate" type="text/html" title="VisCUIT: Visual Auditor for Bias in CNN Image Classifier" /><published>2022-06-18T00:00:00-04:00</published><updated>2022-06-18T00:00:00-04:00</updated><id>http://localhost:4000/papers/viscuit</id><content type="html" xml:base="http://localhost:4000/papers/viscuit">&lt;p&gt;CNN image classifiers are widely used, thanks to their efficiency and accuracy. However, they can suffer from biases that impede their practical applications. Most existing bias investigation techniques are either inapplicable to general image classification tasks or require significant user efforts in perusing all data subgroups to manually specify which data attributes to inspect. We present VISCUIT, an interactive visualization system that reveals how and why a CNN classifier is biased. VISCUIT visually summarizes the subgroups on which the classifier underperforms and helps users discover and characterize the cause of the underperformances by revealing image concepts responsible for activating neurons that contribute to misclassifications. VISCUIT runs in modern browsers and is open-source, allowing people to easily access and extend the tool to other model architectures and datasets. VISCUIT is available at the following public demo link: https: //poloclub.github.io/VisCUIT . A video demo is available at https://youtu.be/eNDbSyM4R_4.&lt;/p&gt;</content><author><name>Seongmin Lee</name></author><summary type="html">CNN image classifiers are widely used, thanks to their efficiency and accuracy. However, they can suffer from biases that impede their practical applications. Most existing bias investigation techniques are either inapplicable to general image classification tasks or require significant user efforts in perusing all data subgroups to manually specify which data attributes to inspect. We present VISCUIT, an interactive visualization system that reveals how and why a CNN classifier is biased. VISCUIT visually summarizes the subgroups on which the classifier underperforms and helps users discover and characterize the cause of the underperformances by revealing image concepts responsible for activating neurons that contribute to misclassifications. VISCUIT runs in modern browsers and is open-source, allowing people to easily access and extend the tool to other model architectures and datasets. VISCUIT is available at the following public demo link: https: //poloclub.github.io/VisCUIT . A video demo is available at https://youtu.be/eNDbSyM4R_4.</summary></entry><entry><title type="html">MisVis: Explaining Web Misinformation Connections via Visual Summary</title><link href="http://localhost:4000/papers/misvis_chi" rel="alternate" type="text/html" title="MisVis: Explaining Web Misinformation Connections via Visual Summary" /><published>2022-04-27T00:00:00-04:00</published><updated>2022-04-27T00:00:00-04:00</updated><id>http://localhost:4000/papers/misvis_chi</id><content type="html" xml:base="http://localhost:4000/papers/misvis_chi">&lt;p&gt;Identifying and raising awareness about web misinformation is crucial as the Internet has become a major source of information for many people. We introduce MisVis, a web-based interactive tool that helps users better assess misinformation websites and understand their connections with other misinformation sites through visual explanations. Different from the existing techniques that primarily only focus on alerting users of misinformation, MisVis provides new ways to visualize how the site is involved in spreading information on the web and social media. Through MisVis, we contribute novel interactive visual design: Summary View helps users understand a site’s overall reliability by showing the distributions of its linked websites; Graph View presents users with the connection details of how a site is linked to other misinformation websites. In collaboration with researchers at a large security company, we are working to deploy MisVis as a web browser extension for broader impact.&lt;/p&gt;</content><author><name>Seongmin Lee</name></author><summary type="html">Identifying and raising awareness about web misinformation is crucial as the Internet has become a major source of information for many people. We introduce MisVis, a web-based interactive tool that helps users better assess misinformation websites and understand their connections with other misinformation sites through visual explanations. Different from the existing techniques that primarily only focus on alerting users of misinformation, MisVis provides new ways to visualize how the site is involved in spreading information on the web and social media. Through MisVis, we contribute novel interactive visual design: Summary View helps users understand a site’s overall reliability by showing the distributions of its linked websites; Graph View presents users with the connection details of how a site is linked to other misinformation websites. In collaboration with researchers at a large security company, we are working to deploy MisVis as a web browser extension for broader impact.</summary></entry><entry><title type="html">Multi-EPL: Accurate Multi-source Domain Adaptation</title><link href="http://localhost:4000/papers/multiepl" rel="alternate" type="text/html" title="Multi-EPL: Accurate Multi-source Domain Adaptation" /><published>2021-08-05T00:00:00-04:00</published><updated>2021-08-05T00:00:00-04:00</updated><id>http://localhost:4000/papers/multiepl</id><content type="html" xml:base="http://localhost:4000/papers/multiepl">&lt;p&gt;Given multiple source datasets with labels, how can we train a target model with no labeled data? Multi-source domain adaptation (MSDA) aims to train a model using multiple source datasets different from a target dataset in the absence of target data labels. MSDA is a crucial problem applicable to many practical cases where labels for the target data are unavailable due to privacy issues. Existing MSDA frameworks are limited since they align data without considering labels of the features of each domain. They also do not fully utilize the target data without labels and rely on limited feature extraction with a single extractor. In this paper, we propose Multi-EPL, a novel method for MSDA. Multi-EPL exploits label-wise moment matching to align the conditional distributions of the features for the labels, uses pseudolabels for the unavailable target labels, and introduces an ensemble of multiple feature extractors for accurate domain adaptation. Extensive experiments show that Multi-EPL provides the state-of-the-art performance for MSDA tasks in both image domains and text domains, improving the accuracy by up to 13.20%.&lt;/p&gt;</content><author><name>Seongmin Lee</name></author><summary type="html">Given multiple source datasets with labels, how can we train a target model with no labeled data? Multi-source domain adaptation (MSDA) aims to train a model using multiple source datasets different from a target dataset in the absence of target data labels. MSDA is a crucial problem applicable to many practical cases where labels for the target data are unavailable due to privacy issues. Existing MSDA frameworks are limited since they align data without considering labels of the features of each domain. They also do not fully utilize the target data without labels and rely on limited feature extraction with a single extractor. In this paper, we propose Multi-EPL, a novel method for MSDA. Multi-EPL exploits label-wise moment matching to align the conditional distributions of the features for the labels, uses pseudolabels for the unavailable target labels, and introduces an ensemble of multiple feature extractors for accurate domain adaptation. Extensive experiments show that Multi-EPL provides the state-of-the-art performance for MSDA tasks in both image domains and text domains, improving the accuracy by up to 13.20%.</summary></entry></feed>