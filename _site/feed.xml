<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-06-16T16:46:37-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Seongmin Lee</title><author><name>Seongmin Lee</name><email>seongmin@gatech.edu</email></author><entry><title type="html">Interpretation Meets Safety: A Survey on Interpretation Methods and Tools for Improving LLM Safety</title><link href="http://localhost:4000/papers/llm-interpretation-safety-survey" rel="alternate" type="text/html" title="Interpretation Meets Safety: A Survey on Interpretation Methods and Tools for Improving LLM Safety" /><published>2025-06-05T00:00:00-07:00</published><updated>2025-06-05T00:00:00-07:00</updated><id>http://localhost:4000/papers/llm-interpretation-safety-survey</id><content type="html" xml:base="http://localhost:4000/papers/llm-interpretation-safety-survey">&lt;p&gt;As large language models (LLMs) see wider real-world use, understanding and mitigating their unsafe behaviors is critical. Interpretation techniques can reveal causes of unsafe outputs and guide safety, but such connections with safety are often overlooked in prior surveys. We present the first survey that bridges this gap, introducing a unified framework that connects safety-focused interpretation methods, the safety enhancements they inform, and the tools that operationalize them. Our novel taxonomy, organized by LLM workflow stages, summarizes nearly 70 works at their intersections. We conclude with open challenges and future directions. This timely survey helps researchers and practitioners navigate key advancements for safer, more interpretable LLMs.&lt;/p&gt;</content><author><name>Seongmin Lee</name></author><summary type="html">As large language models (LLMs) see wider real-world use, understanding and mitigating their unsafe behaviors is critical. Interpretation techniques can reveal causes of unsafe outputs and guide safety, but such connections with safety are often overlooked in prior surveys. We present the first survey that bridges this gap, introducing a unified framework that connects safety-focused interpretation methods, the safety enhancements they inform, and the tools that operationalize them. Our novel taxonomy, organized by LLM workflow stages, summarizes nearly 70 works at their intersections. We conclude with open challenges and future directions. This timely survey helps researchers and practitioners navigate key advancements for safer, more interpretable LLMs.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/papers/25-llm-interpretation-safety-survey.png" /><media:content medium="image" url="http://localhost:4000/images/papers/25-llm-interpretation-safety-survey.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Probing LLM Hallucination from Within: Perturbation-Driven Approach via Internal Knowledge</title><link href="http://localhost:4000/papers/llm-hallucination-probing" rel="alternate" type="text/html" title="Probing LLM Hallucination from Within: Perturbation-Driven Approach via Internal Knowledge" /><published>2025-06-02T00:00:00-07:00</published><updated>2025-06-02T00:00:00-07:00</updated><id>http://localhost:4000/papers/llm-probing</id><content type="html" xml:base="http://localhost:4000/papers/llm-hallucination-probing">&lt;p&gt;LLM hallucination, where unfaithful text is generated, presents a critical challenge for LLMs’ practical applications. Current detection methods often resort to external knowledge, LLM fine-tuning, or supervised training with large hallucination-labeled datasets. Moreover, these approaches do not distinguish between different types of hallucinations, which is crucial for enhancing detection performance. To address such limitations, we introduce hallucination probing, a new task that classifies LLM-generated text into three categories: aligned, misaligned, and fabricated. Driven by our novel discovery that perturbing key entities in prompts affects LLM’s generation of these three types of text differently, we propose SHINE, a novel hallucination probing method that does not require external knowledge, supervised training, or LLM fine-tuning. SHINE is effective in hallucination probing across three modern LLMs, and achieves state-of-the-art performance in hallucination detection, outperforming seven competing methods across four datasets and four LLMs, underscoring the importance of probing for accurate detection.&lt;/p&gt;</content><author><name>Seongmin Lee</name></author><summary type="html">LLM hallucination, where unfaithful text is generated, presents a critical challenge for LLMs’ practical applications. Current detection methods often resort to external knowledge, LLM fine-tuning, or supervised training with large hallucination-labeled datasets. Moreover, these approaches do not distinguish between different types of hallucinations, which is crucial for enhancing detection performance. To address such limitations, we introduce hallucination probing, a new task that classifies LLM-generated text into three categories: aligned, misaligned, and fabricated. Driven by our novel discovery that perturbing key entities in prompts affects LLM’s generation of these three types of text differently, we propose SHINE, a novel hallucination probing method that does not require external knowledge, supervised training, or LLM fine-tuning. SHINE is effective in hallucination probing across three modern LLMs, and achieves state-of-the-art performance in hallucination detection, outperforming seven competing methods across four datasets and four LLMs, underscoring the importance of probing for accurate detection.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/papers/25-llm-hallucination-probing.png" /><media:content medium="image" url="http://localhost:4000/images/papers/25-llm-hallucination-probing.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">LLM Attributor: Interactive Visual Attribution for LLM Generation</title><link href="http://localhost:4000/papers/llm-attributor" rel="alternate" type="text/html" title="LLM Attributor: Interactive Visual Attribution for LLM Generation" /><published>2024-11-19T00:00:00-08:00</published><updated>2024-11-19T00:00:00-08:00</updated><id>http://localhost:4000/papers/llmattributor</id><content type="html" xml:base="http://localhost:4000/papers/llm-attributor">&lt;p&gt;While large language models (LLMs) have shown remarkable capability to generate convincing text across diverse domains, concerns around its potential risks have highlighted the importance of understanding the rationale behind text generation. We present LLM Attributor, a Python library that provides interactive visualizations for training data attribution of an LLM’s text generation. Our library offers a new way to quickly attribute an LLM’s text generation to training data points to inspect model behaviors, enhance its trustworthiness, and compare model-generated text with user-provided text. We describe the visual and interactive design of our tool and highlight usage scenarios for LLaMA2 models fine-tuned with two different datasets: online articles about recent disasters and finance-related question-answer pairs. Thanks to LLM Attributor’s broad support for computational notebooks, users can easily integrate it into their workflow to interactively visualize attributions of their models. For easier access and extensibility, we open-source LLM Attributor at https://github.com/poloclub/ LLM-Attribution. The video demo is available at https://youtu.be/mIG2MDQKQxM.&lt;/p&gt;</content><author><name>Seongmin Lee</name></author><summary type="html">While large language models (LLMs) have shown remarkable capability to generate convincing text across diverse domains, concerns around its potential risks have highlighted the importance of understanding the rationale behind text generation. We present LLM Attributor, a Python library that provides interactive visualizations for training data attribution of an LLM’s text generation. Our library offers a new way to quickly attribute an LLM’s text generation to training data points to inspect model behaviors, enhance its trustworthiness, and compare model-generated text with user-provided text. We describe the visual and interactive design of our tool and highlight usage scenarios for LLaMA2 models fine-tuned with two different datasets: online articles about recent disasters and finance-related question-answer pairs. Thanks to LLM Attributor’s broad support for computational notebooks, users can easily integrate it into their workflow to interactively visualize attributions of their models. For easier access and extensibility, we open-source LLM Attributor at https://github.com/poloclub/ LLM-Attribution. The video demo is available at https://youtu.be/mIG2MDQKQxM.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/papers/24-llm-attributor.png" /><media:content medium="image" url="http://localhost:4000/images/papers/24-llm-attributor.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Effective Guidance for Model Attention with Simple Yes-no Annotations</title><link href="http://localhost:4000/papers/crayon" rel="alternate" type="text/html" title="Effective Guidance for Model Attention with Simple Yes-no Annotations" /><published>2024-10-28T00:00:00-07:00</published><updated>2024-10-28T00:00:00-07:00</updated><id>http://localhost:4000/papers/crayon</id><content type="html" xml:base="http://localhost:4000/papers/crayon">&lt;p&gt;Modern deep learning models often make predictions by focusing on irrelevant areas, leading to biased performance and limited generalization. Existing methods aimed at rectifying model attention require explicit labels for irrelevant areas or complex pixel-wise ground truth attention maps. We present CRAYON (Correcting Reasoning with Annotations of Yes Or No), offering effective, scalable, and practical solutions to rectify model attention using simple yes-no annotations. CRAYON empowers classical and modern model interpretation techniques to identify and guide model reasoning: CRAYON-ATTENTION directs classic interpretations based on saliency maps to focus on relevant image regions, while CRAYON-PRUNING removes irrelevant neurons identified by modern concept-based methods to mitigate their influence. Through extensive experiments with both quantitative and human evaluation, we showcase CRAYON’s effectiveness, scalability, and practicality in refining model attention. CRAYON achieves state-of-the-art performance, outperforming 12 methods across 3 benchmark datasets, surpassing approaches that require more complex annotations.&lt;/p&gt;</content><author><name>Seongmin Lee</name></author><summary type="html">Modern deep learning models often make predictions by focusing on irrelevant areas, leading to biased performance and limited generalization. Existing methods aimed at rectifying model attention require explicit labels for irrelevant areas or complex pixel-wise ground truth attention maps. We present CRAYON (Correcting Reasoning with Annotations of Yes Or No), offering effective, scalable, and practical solutions to rectify model attention using simple yes-no annotations. CRAYON empowers classical and modern model interpretation techniques to identify and guide model reasoning: CRAYON-ATTENTION directs classic interpretations based on saliency maps to focus on relevant image regions, while CRAYON-PRUNING removes irrelevant neurons identified by modern concept-based methods to mitigate their influence. Through extensive experiments with both quantitative and human evaluation, we showcase CRAYON’s effectiveness, scalability, and practicality in refining model attention. CRAYON achieves state-of-the-art performance, outperforming 12 methods across 3 benchmark datasets, surpassing approaches that require more complex annotations.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/featured/23-crayon.png" /><media:content medium="image" url="http://localhost:4000/images/featured/23-crayon.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">LLM Hallucination Reasoning with Zero-shot Knowledge Test</title><link href="http://localhost:4000/papers/hallreasoning" rel="alternate" type="text/html" title="LLM Hallucination Reasoning with Zero-shot Knowledge Test" /><published>2024-10-09T00:00:00-07:00</published><updated>2024-10-09T00:00:00-07:00</updated><id>http://localhost:4000/papers/hallreasoning</id><content type="html" xml:base="http://localhost:4000/papers/hallreasoning">&lt;p&gt;LLM hallucination, where LLMs occasionally generate unfaithful text, poses significant challenges for their practical applications. Most existing detection methods rely on external knowledge, LLM fine-tuning, or hallucination-labeled datasets, and they do not distinguish between different types of hallucinations, which are crucial for improving detection performance. We introduce a new task, Hallucination Reasoning, which classifies LLM-generated text into one of three categories: aligned, misaligned, and fabricated. Our novel zero-shot method assesses whether LLM has enough knowledge about a given prompt and text. Our experiments conducted on new datasets demonstrate the effectiveness of our method in hallucination reasoning and underscore its importance for enhancing detection performance.&lt;/p&gt;</content><author><name>Seongmin Lee</name></author><summary type="html">LLM hallucination, where LLMs occasionally generate unfaithful text, poses significant challenges for their practical applications. Most existing detection methods rely on external knowledge, LLM fine-tuning, or hallucination-labeled datasets, and they do not distinguish between different types of hallucinations, which are crucial for improving detection performance. We introduce a new task, Hallucination Reasoning, which classifies LLM-generated text into one of three categories: aligned, misaligned, and fabricated. Our novel zero-shot method assesses whether LLM has enough knowledge about a given prompt and text. Our experiments conducted on new datasets demonstrate the effectiveness of our method in hallucination reasoning and underscore its importance for enhancing detection performance.</summary></entry><entry><title type="html">Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion</title><link href="http://localhost:4000/papers/diffusion-explainer" rel="alternate" type="text/html" title="Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion" /><published>2024-08-02T00:00:00-07:00</published><updated>2024-08-02T00:00:00-07:00</updated><id>http://localhost:4000/papers/diffusionexplainer</id><content type="html" xml:base="http://localhost:4000/papers/diffusion-explainer">&lt;p&gt;Diffusion-based generative models’ impressive ability to create convincing images has captured global attention. However, their complex internal structures and operations often make them difficult for non-experts to understand. We present Diffusion Explainer, the first interactive visualization tool that explains how Stable Diffusion transforms text prompts into images. Diffusion Explainer tightly integrates a visual overview of Stable Diffusion’s complex components with detailed explanations of their underlying operations, enabling users to fluidly transition between multiple levels of abstraction through animations and interactive elements. By comparing the evolutions of image representations guided by two related text prompts over refinement timesteps, users can discover the impact of prompts on image generation. Diffusion Explainer runs locally in users’ web browsers without the need for installation or specialized hardware, broadening the public’s education access to modern AI techniques. Our open-sourced tool is available at: https://poloclub.github.io/diffusion-explainer/. A video demo is available at https://youtu.be/Zg4gxdIWDds.&lt;/p&gt;</content><author><name>Seongmin Lee</name></author><summary type="html">Diffusion-based generative models’ impressive ability to create convincing images has captured global attention. However, their complex internal structures and operations often make them difficult for non-experts to understand. We present Diffusion Explainer, the first interactive visualization tool that explains how Stable Diffusion transforms text prompts into images. Diffusion Explainer tightly integrates a visual overview of Stable Diffusion’s complex components with detailed explanations of their underlying operations, enabling users to fluidly transition between multiple levels of abstraction through animations and interactive elements. By comparing the evolutions of image representations guided by two related text prompts over refinement timesteps, users can discover the impact of prompts on image generation. Diffusion Explainer runs locally in users’ web browsers without the need for installation or specialized hardware, broadening the public’s education access to modern AI techniques. Our open-sourced tool is available at: https://poloclub.github.io/diffusion-explainer/. A video demo is available at https://youtu.be/Zg4gxdIWDds.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/featured/23-diffusion-explainer-short.gif" /><media:content medium="image" url="http://localhost:4000/images/featured/23-diffusion-explainer-short.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Transformer Explainer: Interactive Learning of Text-Generative Models</title><link href="http://localhost:4000/papers/transformer-explainer" rel="alternate" type="text/html" title="Transformer Explainer: Interactive Learning of Text-Generative Models" /><published>2024-08-01T00:00:00-07:00</published><updated>2024-08-01T00:00:00-07:00</updated><id>http://localhost:4000/papers/transformerexplainer</id><content type="html" xml:base="http://localhost:4000/papers/transformer-explainer">&lt;p&gt;While large language models (LLMs) have shown remarkable capability to generate convincing text across diverse domains, concerns around its potential risks have highlighted the importance of understanding the rationale behind text generation. We present LLM Attributor, a Python library that provides interactive visualizations for training data attribution of an LLM’s text generation. Our library offers a new way to quickly attribute an LLM’s text generation to training data points to inspect model behaviors, enhance its trustworthiness, and compare model-generated text with user-provided text. We describe the visual and interactive design of our tool and highlight usage scenarios for LLaMA2 models fine-tuned with two different datasets: online articles about recent disasters and finance-related question-answer pairs. Thanks to LLM Attributor’s broad support for computational notebooks, users can easily integrate it into their workflow to interactively visualize attributions of their models. For easier access and extensibility, we open-source LLM Attributor at https://github.com/poloclub/ LLM-Attribution. The video demo is available at https://youtu.be/mIG2MDQKQxM.&lt;/p&gt;</content><author><name>Aeree Cho</name></author><summary type="html">While large language models (LLMs) have shown remarkable capability to generate convincing text across diverse domains, concerns around its potential risks have highlighted the importance of understanding the rationale behind text generation. We present LLM Attributor, a Python library that provides interactive visualizations for training data attribution of an LLM’s text generation. Our library offers a new way to quickly attribute an LLM’s text generation to training data points to inspect model behaviors, enhance its trustworthiness, and compare model-generated text with user-provided text. We describe the visual and interactive design of our tool and highlight usage scenarios for LLaMA2 models fine-tuned with two different datasets: online articles about recent disasters and finance-related question-answer pairs. Thanks to LLM Attributor’s broad support for computational notebooks, users can easily integrate it into their workflow to interactively visualize attributions of their models. For easier access and extensibility, we open-source LLM Attributor at https://github.com/poloclub/ LLM-Attribution. The video demo is available at https://youtu.be/mIG2MDQKQxM.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/papers/24-transformer-explainer.png" /><media:content medium="image" url="http://localhost:4000/images/papers/24-transformer-explainer.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">SuperNOVA: Design Strategies and Opportunities for Interactive Visualization in Computational Notebooks</title><link href="http://localhost:4000/papers/supernova" rel="alternate" type="text/html" title="SuperNOVA: Design Strategies and Opportunities for Interactive Visualization in Computational Notebooks" /><published>2024-03-28T00:00:00-07:00</published><updated>2024-03-28T00:00:00-07:00</updated><id>http://localhost:4000/papers/supernova</id><content type="html" xml:base="http://localhost:4000/papers/supernova">&lt;p&gt;Computational notebooks such as Jupyter Notebook have become data scientists’ de facto programming environments. Many visualization researchers and practitioners have developed interactive visualization tools that support notebooks. However, little is known about the appropriate design of visual analytics (VA) tools in notebooks. To bridge this critical research gap, we investigate the design strategies in this space by analyzing 159 notebook VA tools and their users’ feedback. Our analysis encompasses 62 systems from academic papers and 103 systems sourced from a pool of 55k notebooks containing interactive visualizations that we obtain via scraping 8.6 million notebooks on GitHub. We also examine findings from 15 user studies and user feedback in 379 GitHub issues. Through this work, we identify unique design opportunities and considerations for future notebook VA tools, such as using and manipulating multimodal data in notebooks as well as balancing the degree of visualization-notebook integration. Finally, we develop SuperNOVA, an open-source interactive tool to help researchers explore existing notebook VA tools and search for related work.&lt;/p&gt;</content><author><name>Zijie J. Wang</name></author><summary type="html">Computational notebooks such as Jupyter Notebook have become data scientists’ de facto programming environments. Many visualization researchers and practitioners have developed interactive visualization tools that support notebooks. However, little is known about the appropriate design of visual analytics (VA) tools in notebooks. To bridge this critical research gap, we investigate the design strategies in this space by analyzing 159 notebook VA tools and their users’ feedback. Our analysis encompasses 62 systems from academic papers and 103 systems sourced from a pool of 55k notebooks containing interactive visualizations that we obtain via scraping 8.6 million notebooks on GitHub. We also examine findings from 15 user studies and user feedback in 379 GitHub issues. Through this work, we identify unique design opportunities and considerations for future notebook VA tools, such as using and manipulating multimodal data in notebooks as well as balancing the degree of visualization-notebook integration. Finally, we develop SuperNOVA, an open-source interactive tool to help researchers explore existing notebook VA tools and search for related work.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/papers/23-supernova.png" /><media:content medium="image" url="http://localhost:4000/images/papers/23-supernova.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">ClickDiffusion: Harnessing LLMs for Interactive Precise Image Editing</title><link href="http://localhost:4000/papers/clickdiffusion" rel="alternate" type="text/html" title="ClickDiffusion: Harnessing LLMs for Interactive Precise Image Editing" /><published>2024-03-07T00:00:00-08:00</published><updated>2024-03-07T00:00:00-08:00</updated><id>http://localhost:4000/papers/clickdiffusion</id><content type="html" xml:base="http://localhost:4000/papers/clickdiffusion">&lt;p&gt;Recently, researchers have proposed powerful systems for generating and manipulating images using natural language instructions. However, it is difficult to precisely specify many common classes of image transformations with text alone. For example, a user may wish to change the location and breed of a particular dog in an image with several similar dogs. This task is quite difficult with natural language alone, and would require a user to write a laboriously complex prompt that both disambiguates the target dog and describes the destination. We propose ClickDiffusion, a system for precise image manipulation and generation that combines natural language instructions with visual feedback provided by the user through a direct manipulation interface. We demonstrate that by serializing both an image and a multi-modal instruction into a textual representation it is possible to leverage LLMs to perform precise transformations of the layout and appearance of an image. Code available at https://github.com/poloclub/ClickDiffusion.&lt;/p&gt;</content><author><name>Alec Helbling</name></author><summary type="html">Recently, researchers have proposed powerful systems for generating and manipulating images using natural language instructions. However, it is difficult to precisely specify many common classes of image transformations with text alone. For example, a user may wish to change the location and breed of a particular dog in an image with several similar dogs. This task is quite difficult with natural language alone, and would require a user to write a laboriously complex prompt that both disambiguates the target dog and describes the destination. We propose ClickDiffusion, a system for precise image manipulation and generation that combines natural language instructions with visual feedback provided by the user through a direct manipulation interface. We demonstrate that by serializing both an image and a multi-modal instruction into a textual representation it is possible to leverage LLMs to perform precise transformations of the layout and appearance of an image. Code available at https://github.com/poloclub/ClickDiffusion.</summary></entry><entry><title type="html">UniTable: Towards a Unified Framework for Table Structure Recognition via Self-Supervised Pretraining</title><link href="http://localhost:4000/papers/unitable" rel="alternate" type="text/html" title="UniTable: Towards a Unified Framework for Table Structure Recognition via Self-Supervised Pretraining" /><published>2024-03-07T00:00:00-08:00</published><updated>2024-03-07T00:00:00-08:00</updated><id>http://localhost:4000/papers/unitable</id><content type="html" xml:base="http://localhost:4000/papers/unitable">&lt;p&gt;Tables convey factual and quantitative data with implicit conventions created by humans that are often challenging for machines to parse. Prior work on table structure recognition (TSR) has mainly centered around complex task-specific combinations of available inputs and tools. We present UniTable, a training framework that unifies both the training paradigm and training objective of TSR. Its training paradigm combines the simplicity of purely pixel-level inputs with the effectiveness and scalability empowered by self-supervised pretraining (SSP) from diverse unannotated tabular images. Our framework unifies the training objectives of all three TSR tasks - extracting table structure, cell content, and cell bounding box (bbox) - into a unified task-agnostic training objective: language modeling. Extensive quantitative and qualitative analyses highlight UniTable’s state-of-the-art (SOTA) performance on four of the largest TSR datasets. To promote reproducible research, enhance transparency, and SOTA innovations, we open-source our code at https://github.com/poloclub/unitable and release the first-of-its-kind Jupyter Notebook of the whole inference pipeline, fine-tuned across multiple TSR datasets, supporting all three TSR tasks.&lt;/p&gt;</content><author><name>ShengYun Peng</name></author><summary type="html">Tables convey factual and quantitative data with implicit conventions created by humans that are often challenging for machines to parse. Prior work on table structure recognition (TSR) has mainly centered around complex task-specific combinations of available inputs and tools. We present UniTable, a training framework that unifies both the training paradigm and training objective of TSR. Its training paradigm combines the simplicity of purely pixel-level inputs with the effectiveness and scalability empowered by self-supervised pretraining (SSP) from diverse unannotated tabular images. Our framework unifies the training objectives of all three TSR tasks - extracting table structure, cell content, and cell bounding box (bbox) - into a unified task-agnostic training objective: language modeling. Extensive quantitative and qualitative analyses highlight UniTable’s state-of-the-art (SOTA) performance on four of the largest TSR datasets. To promote reproducible research, enhance transparency, and SOTA innovations, we open-source our code at https://github.com/poloclub/unitable and release the first-of-its-kind Jupyter Notebook of the whole inference pipeline, fine-tuned across multiple TSR datasets, supporting all three TSR tasks.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/papers/24-unitable.png" /><media:content medium="image" url="http://localhost:4000/images/papers/24-unitable.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>